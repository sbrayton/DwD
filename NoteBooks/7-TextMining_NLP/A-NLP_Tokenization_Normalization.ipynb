{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Make the graphs a bit prettier, and bigger\n",
    "plt.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing NLTK toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting let's install the NLTK library (http://www.nltk.org/), by typing the following commands in vagrant terminal.\n",
    "\n",
    "* Install Numpy: `sudo -H pip3 install -U numpy`\n",
    "* Install NLTK: `sudo -H pip3 install -U nltk`\n",
    "* Install Tkinter: `sudo -H apt-get -y install python3-tk`\n",
    "\n",
    "Test that the library is installed properly by executing the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the NLTK toolkit is installed, we need to install the NLTK data: \n",
    "\n",
    "`sudo python -m nltk.downloader -d /usr/share/nltk_data all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package hmm_treebank_pos_tagger to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/hmm_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package panlex_lite to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/panlex_lite.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "!sudo python3 -m nltk.downloader -d /usr/share/nltk_data all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra NLTK resources\n",
    "\n",
    "NLTK also comes with some of the files from Project Gutenberg already included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34110"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice  = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
    "len(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145735"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is one thing to automatically detect that a particular word occurs in a text, and to display some words that appear in the same context. However, we can also determine the location of a word in the text: how many words from the beginning it appears. This positional information can be displayed using a dispersion plot. Each stripe represents an instance of a word, and each row represents the entire text. You can produce this plot as shown below. You might like to try more words (e.g., liberty, constitution), and different texts. Can you predict the dispersion of a word before you view it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlIAAAGHCAYAAAB7xLxyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEwiBQAwiRQAwCRSAACTSAEATAFPRHmNWdnPEwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f05e08bf2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text4 is the inauguration addresses\n",
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\", \"world\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "* Pick your own book and create a dispersion plot for your keywords of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "alice = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
    "t_alice = nltk.Text(alice)\n",
    "t_alice.dispersion_plot([\"rabbit\", \"Alice\", \"hole\", \"Queen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text7.dispersion_plot([\"she\", \"he\", \"He\", \"She\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Text: Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by fetching a piece of text. We will go to [Project Gutenberg](https://www.gutenberg.org/) and fetch the text for \"The origin of species\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# The origin of species\n",
    "# Original at http://www.gutenberg.org/cache/epub/1228/pg1228.txt but there seems to be \n",
    "# some problem with downloading from Amazon EC2\n",
    "# url = \"http://www.gutenberg.org/cache/epub/1228/pg1228.txt\"\n",
    "url = \"https://dl.dropboxusercontent.com/u/16006464/DwD_Winter2015/1228.txt\"\n",
    "\n",
    "# Get the URL, do not check the SSL certificate\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Get the text\n",
    "content = resp.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951128\n"
     ]
    }
   ],
   "source": [
    "# The text contains template stuff at the beginning and end. Let's get rid of these\n",
    "start_phrase = \"*** START OF THIS PROJECT GUTENBERG EBOOK ON THE ORIGIN OF SPECIES ***\"\n",
    "end_phrase = \"*** END OF THIS PROJECT GUTENBERG EBOOK ON THE ORIGIN OF SPECIES ***\"\n",
    "s = content.index(start_phrase)\n",
    "e = content.index(end_phrase)\n",
    "true_content = content[s+len(start_phrase):e]\n",
    "\n",
    "# Approximate bytes of text\n",
    "print len(true_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency distributions, Zipf's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our first text ready to be analyzed. Let's first do some analysis of the words that appear in this classic text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 13908 samples and 155443 outcomes>\n"
     ]
    }
   ],
   "source": [
    "tokens = true_content.split()\n",
    "\n",
    "# The nltk.Text object will offer us many useful functions for text analysis\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "# Frequency analysis for words of interest\n",
    "fdist = text.vocab()\n",
    "\n",
    "# Number of unique and total words in the text\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the frequencies of some words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fdist[\"species\"]\n",
    "print fdist[\"sexual\"]\n",
    "print fdist[\"origin\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual words of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's see a few more words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, that is not very useful. These are all words that are needed by every single English text. Only the world \"species\" seems to have some meaning. The rest of the words tell us nothing about the text; they're just English \"plumbing.\"\n",
    "\n",
    "What proportion of the text is taken up with such words? We can generate a cumulative frequency plot for these words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 50 words account for nearly half the book! (If you rememeber, we had 155443 words in the book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the frequent words don't help us, how about the words that occur once only, the so-called hapaxes? View them by typing `fdist.hapaxes()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist.hapaxes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we have a problem. We generated the words of the text by doing a simple `split()`. So our \"words\" also contain punctuation, and words with different capitalization are considered difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in order to to proper analysis we need to remove from the document all the punctuation. However, keeping only alphanumeric characters will break things like `B.Sc.` `N.Y.U.` and so on. The process of properly splitting the document into appropriate basic elements is called `tokenization`.\n",
    "\n",
    "NLTK gives us a (set of ) function call(s) that can do the tokenization (see also http://www.nltk.org/_modules/nltk/tokenize.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'bagels', 'cost', '$', '2.88', 'in', 'New', 'York', '.', 'Hey', 'Prof.', 'Ipeirotis', ',', 'please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', 'PS', ':', 'You', 'have', 'a', 'Ph.D.', ',', 'you', 'can', 'handle', 'this', ',', 'right', '?']\n"
     ]
    }
   ],
   "source": [
    "example = '''Good bagels cost $2.88 in New York.  \n",
    "    Hey Prof. Ipeirotis, please buy me two of them.\n",
    "    \n",
    "    Thanks.\n",
    "    \n",
    "    PS: You have a Ph.D., you can handle this, right?'''\n",
    "\n",
    "print nltk.word_tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = \"On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.\"\n",
    "print nltk.word_tokenize(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s2 = \"\\\"We beat some pretty good teams to get here,\\\" Slocum said.\"\n",
    "print nltk.word_tokenize(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3 = \"Well, we couldn't have this predictable, cliche-ridden, \\\"Touched by an Angel\\\" (a show creator John Masius worked on) wanna-be if she didn't.\"\n",
    "print nltk.word_tokenize(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s4 = \"I cannot cannot work under these conditions!\"\n",
    "print nltk.word_tokenize(s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s5 = \"The company spent $30,000,000 last year.\"\n",
    "print nltk.word_tokenize(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s6 = \"The company spent 40.75% of its income last year.\"\n",
    "print nltk.word_tokenize(s6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s7 = \"He arrived at 3:00 pm.\"\n",
    "print nltk.word_tokenize(s7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s8 = \"I bought these items: books, pencils, and pens.\"\n",
    "print nltk.word_tokenize(s8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s9 = \"Though there were 150, 100 of them were old.\"\n",
    "print nltk.word_tokenize(s9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s10 = \"There were 300,000, but that wasn't enough.\"\n",
    "print nltk.word_tokenize(s10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's repeat the process now for our original text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We tokenize and we also convert to lowercase for further normalization\n",
    "tokens = nltk.word_tokenize(true_content.lower())\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "# Frequency analysis for words of interest\n",
    "fdist = text.vocab()\n",
    "\n",
    "# Number of unique and total words in the text\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went from `13908 samples and 155443 outcomes` to `7687 samples and 175682 outcomes`. In other words, we have now 7687 unique tokens, and a set of 175682 tokens, as punctuation characters are now separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fdist.hapaxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len (fdist.hapaxes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So out of the 7687 unique words, 2666 of them appear only once in the text. But these are only 2666 out of the total of 175682 words in the text. This is ~1.5% of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization process can also work on separating sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = '''Good bagels cost $2.88 in N.Y.C. Hey Prof. Ipeirotis, please buy me two of them.\n",
    "    \n",
    "    Thanks.\n",
    "    \n",
    "    PS: You have a Ph.D. you can handle this, right?'''\n",
    "\n",
    "print nltk.sent_tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zipf's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipf's law says that the frequencies of words in text follow a power-law: A few words account for a big fraction of the text (the very frequent ones, usually just the \"plumping\" of English), and a large fraction of the unique vocabularly (the \"hapaxes\") appear very infrequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist.plot(100, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist.plot(100, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization: Stopwords\n",
    "\n",
    "NLTK contains a corpus of stopwords, that is, high-frequency words like `the`, `to` and `also` that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print stopwords.words('english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to remove the words in a text are in the stopwords list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mystops = []\n",
    "mystops.append('one')\n",
    "mystops.append('may')\n",
    "mystops.append('would')\n",
    "mystops.append('many')\n",
    "\n",
    "def remove_stopwords(text, hapaxes):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w.lower() for w in text \n",
    "               if w.lower() not in stopwords \\ # w should not be in NLTK stopwords\n",
    "                   and w.lower() not in mystops # w should not be in custom stop word list\n",
    "                   and w.isalpha() \\ # w should consists of letters, not numbers, not punctuation\n",
    "                   and w.lower() not in hapaxes] # w should have frequency > 1 \n",
    "    return nltk.Text(content)\n",
    "\n",
    "text_nostopwords = remove_stopwords(text, fdist.hapaxes())\n",
    "fdist_nostopwords = text_nostopwords.vocab()\n",
    "print fdist_nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fdist_nostopwords.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist_nostopwords.plot(100, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization: Stemming \n",
    "\n",
    "So far, the only normalization that we did is to convert text to lowercase before doing anything with its words, e.g. `set(w.lower() for w in text)`. By using lower(), we have normalized the text to lowercase so that the distinction between `The` and `the` is ignored. Often we want to go further than this, and strip off any affixes, a task known as **stemming**. \n",
    "\n",
    "NLTK includes several off-the-shelf stemmers, and if you ever need a stemmer you should use one of these. The Porter stemmer is a very well-known stemmer, and should suffice for most of our applications. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tok = nltk.word_tokenize(raw)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "stemmed =  [porter.stem(t) for t in tok]\n",
    "\n",
    "# This idiom concatenates all the words in a list. \n",
    "# The call is s.join(list), where we join the element \n",
    "# of the list using the string s as the concatenacting character\n",
    "print \" \".join(stemmed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization: Lemmatization\n",
    "\n",
    "A further step in the normalization process is to make sure that the resulting form is a known word in a dictionary, a task known as **lemmatization**. The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary. This additional checking process makes the lemmatizer slower than the above stemmers. Notice that it doesn't handle lying, but it converts women to woman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tok = nltk.word_tokenize(raw)\n",
    "\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatized =  [wnl.lemmatize(t) for t in tok]\n",
    "print \" \".join(lemmatized) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is this WordNet? It is one of the most useful resources for anyone interested in analyzing text at a more semantic level than simply frequency counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet\n",
    "\n",
    "WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure. NLTK includes the English WordNet, with 155,287 words and 117,659 synonym sets. We'll begin by looking at synonyms and how they are accessed in WordNet.\n",
    "\n",
    "####  Senses and Synonyms\n",
    "\n",
    "Consider the sentence below. If we replace the word motorcar in by automobile, the meaning of the sentence stays pretty much the same:\n",
    "\n",
    "* Benz is credited with the invention of the motorcar.\n",
    "* Benz is credited with the invention of the automobile.\n",
    "\n",
    "Since everything else in the sentence has remained unchanged, we can conclude that the words motorcar and automobile have the same meaning, i.e. they are **synonyms**. We can explore these words with the help of WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, motorcar has just one possible meaning and it is identified as car.n.01, the first noun sense of car. The entity car.n.01 is called a **synset**, or **\"synonym set\"**, a collection of synonymous words (or \"lemmas\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'car', u'auto', u'automobile', u'machine', u'motorcar']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word of a synset can have several meanings, e.g., car can also signify a train carriage, a gondola, or an elevator car. However, we are only interested in the single meaning that is common to all words of the above synset. Synsets also come with a prose definition and some example sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'he needs a car to get to work']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although definitions help humans to understand the intended meaning of a synset, the words of the synset are often more useful for our programs. To eliminate ambiguity, we will identify these words as car.n.01.automobile, car.n.01.motorcar, and so on. This pairing of a synset with a word is called a **lemma**. We can get all the lemmas for a given synset, look up a particular lemma, get the synset corresponding to a lemma, and get the \"name\" of a lemma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.01.auto'),\n",
       " Lemma('car.n.01.automobile'),\n",
       " Lemma('car.n.01.machine'),\n",
       " Lemma('car.n.01.motorcar')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemmas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemma('car.n.01.automobile')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('car.n.01')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile').synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'automobile'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile').name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's analyze the word `car`, which has multiple **senses** (ie., meanings of the word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma name: [u'car', u'auto', u'automobile', u'machine', u'motorcar']\n",
      "Definition: a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "Examples  : [u'he needs a car to get to work']\n",
      "=======================\n",
      "Lemma name: [u'car', u'railcar', u'railway_car', u'railroad_car']\n",
      "Definition: a wheeled vehicle adapted to the rails of railroad\n",
      "Examples  : [u'three cars had jumped the rails']\n",
      "=======================\n",
      "Lemma name: [u'car', u'gondola']\n",
      "Definition: the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "Examples  : []\n",
      "=======================\n",
      "Lemma name: [u'car', u'elevator_car']\n",
      "Definition: where passengers ride up and down\n",
      "Examples  : [u'the car was on the top floor']\n",
      "=======================\n",
      "Lemma name: [u'cable_car', u'car']\n",
      "Definition: a conveyance for passengers or freight on a cable railway\n",
      "Examples  : [u'they took a cable car to the top of the mountain']\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "senses = [(s.lemma_names(), s.definition(), s.examples()) for s in wn.synsets('car')]\n",
    "for s in senses:\n",
    "    print \"Lemma name:\", s[0]\n",
    "    print \"Definition:\", s[1]\n",
    "    print \"Examples  :\", s[2]\n",
    "    print \"=======================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma name: [u'bank']\n",
      "Definition: sloping land (especially the slope beside a body of water)\n",
      "Examples  : [u'they pulled the canoe up on the bank', u'he sat on the bank of the river and watched the currents']\n",
      "=======================\n",
      "Lemma name: [u'depository_financial_institution', u'bank', u'banking_concern', u'banking_company']\n",
      "Definition: a financial institution that accepts deposits and channels the money into lending activities\n",
      "Examples  : [u'he cashed a check at the bank', u'that bank holds the mortgage on my home']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: a long ridge or pile\n",
      "Examples  : [u'a huge bank of earth']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: an arrangement of similar objects in a row or in tiers\n",
      "Examples  : [u'he operated a bank of switches']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: a supply or stock held in reserve for future use (especially in emergencies)\n",
      "Examples  : []\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: the funds held by a gambling house or the dealer in some gambling games\n",
      "Examples  : [u'he tried to break the bank at Monte Carlo']\n",
      "=======================\n",
      "Lemma name: [u'bank', u'cant', u'camber']\n",
      "Definition: a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "Examples  : []\n",
      "=======================\n",
      "Lemma name: [u'savings_bank', u'coin_bank', u'money_box', u'bank']\n",
      "Definition: a container (usually with a slot in the top) for keeping money at home\n",
      "Examples  : [u'the coin bank was empty']\n",
      "=======================\n",
      "Lemma name: [u'bank', u'bank_building']\n",
      "Definition: a building in which the business of banking transacted\n",
      "Examples  : [u'the bank is on the corner of Nassau and Witherspoon']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "Examples  : [u'the plane went into a steep bank']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: tip laterally\n",
      "Examples  : [u'the pilot had to bank the aircraft']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: enclose with a bank\n",
      "Examples  : [u'bank roads']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: do business with a bank or keep an account at a bank\n",
      "Examples  : [u'Where do you bank in this town?']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: act as the banker in a game or in gambling\n",
      "Examples  : []\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: be in the banking business\n",
      "Examples  : []\n",
      "=======================\n",
      "Lemma name: [u'deposit', u'bank']\n",
      "Definition: put into a bank account\n",
      "Examples  : [u'She deposits her paycheck every month']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: cover with ashes so to control the rate of burning\n",
      "Examples  : [u'bank a fire']\n",
      "=======================\n",
      "Lemma name: [u'trust', u'swear', u'rely', u'bank']\n",
      "Definition: have confidence or faith in\n",
      "Examples  : [u'We can trust in God', u'Rely on your friends', u'bank on your good education', u\"I swear by my grandmother's recipes\"]\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "## your code here: Analyze the word \"bank\"\n",
    "senses = [(s.lemma_names(), s.definition(), s.examples()) for s in wn.synsets('bank')]\n",
    "for s in senses:\n",
    "    print \"Lemma name:\", s[0]\n",
    "    print \"Definition:\", s[1]\n",
    "    print \"Examples  :\", s[2]\n",
    "    print \"=======================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The WordNet Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet synsets correspond to abstract concepts, and they don't always have corresponding words in English. These concepts are linked together in a hierarchy. Some concepts are very general, such as Entity, State, Event — these are called unique beginners or root synsets. Others, such as gas guzzler and hatchback, are much more specific. A small portion of a concept hierarchy is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.nltk.org/images/wordnet-hierarchy.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyponyms\n",
    "\n",
    "WordNet makes it easy to navigate between concepts. For example, given a concept like motorcar, we can look at the concepts that are more specific; the (immediate) hyponyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "motorcar = wn.synset('car.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('ambulance.n.01')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_of_motorcar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Model_T', u'S.U.V.', u'SUV', u'Stanley_Steamer', u'ambulance', u'beach_waggon', u'beach_wagon', u'bus', u'cab', u'compact', u'compact_car', u'convertible', u'coupe', u'cruiser', u'electric', u'electric_automobile', u'electric_car', u'estate_car', u'gas_guzzler', u'hack', u'hardtop', u'hatchback', u'heap', u'horseless_carriage', u'hot-rod', u'hot_rod', u'jalopy', u'jeep', u'landrover', u'limo', u'limousine', u'loaner', u'minicar', u'minivan', u'pace_car', u'patrol_car', u'phaeton', u'police_car', u'police_cruiser', u'prowl_car', u'race_car', u'racer', u'racing_car', u'roadster', u'runabout', u'saloon', u'secondhand_car', u'sedan', u'sport_car', u'sport_utility', u'sport_utility_vehicle', u'sports_car', u'squad_car', u'station_waggon', u'station_wagon', u'stock_car', u'subcompact', u'subcompact_car', u'taxi', u'taxicab', u'tourer', u'touring_car', u'two-seater', u'used-car', u'waggon', u'wagon']\n"
     ]
    }
   ],
   "source": [
    "print sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypernyms\n",
    "\n",
    "We can also navigate up the hierarchy by visiting hypernyms. Some words have multiple paths, because they can be classified in more than one way. There are two paths between car.n.01 and entity.n.01 because wheeled_vehicle.n.01 can be classified as both a vehicle and a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar.hypernyms()\n",
    "paths = motorcar.hypernym_paths()\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'entity.n.01', u'physical_entity.n.01', u'object.n.01', u'whole.n.02', u'artifact.n.01', u'instrumentality.n.03', u'container.n.01', u'wheeled_vehicle.n.01', u'self-propelled_vehicle.n.01', u'motor_vehicle.n.01', u'car.n.01']\n"
     ]
    }
   ],
   "source": [
    "print [synset.name() for synset in paths[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'entity.n.01', u'physical_entity.n.01', u'object.n.01', u'whole.n.02', u'artifact.n.01', u'instrumentality.n.03', u'conveyance.n.03', u'vehicle.n.01', u'wheeled_vehicle.n.01', u'self-propelled_vehicle.n.01', u'motor_vehicle.n.01', u'car.n.01']\n"
     ]
    }
   ],
   "source": [
    "print [synset.name() for synset in paths[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Lexical Relations: Meronyms, Holonyms, Antonyms, Entailment\n",
    "\n",
    "Hypernyms and hyponyms are called lexical relations because they relate one synset to another. These two relations navigate up and down the \"is-a\" hierarchy. Another important way to navigate the WordNet network is from items to their components (**meronyms**) or to the things they are contained in (**holonyms**). For example, the parts of a tree are its trunk, crown, and so on; the part_meronyms(). The substance a tree is made of includes heartwood and sapwood; the substance_meronyms(). A collection of trees forms a forest; the member_holonyms():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('trunk.n.01')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('forest.n.01')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('heartwood.n.01'), Synset('sapwood.n.01')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('united_states.n.01'), Synset('united_states_army.n.01')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"USA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('alabama.n.01'),\n",
       " Synset('alaska.n.01'),\n",
       " Synset('american_state.n.01'),\n",
       " Synset('arizona.n.01'),\n",
       " Synset('arkansas.n.01'),\n",
       " Synset('california.n.01'),\n",
       " Synset('colony.n.03'),\n",
       " Synset('colorado.n.01'),\n",
       " Synset('connecticut.n.01'),\n",
       " Synset('connecticut.n.02'),\n",
       " Synset('dakota.n.02'),\n",
       " Synset('delaware.n.04'),\n",
       " Synset('district_of_columbia.n.01'),\n",
       " Synset('east.n.03'),\n",
       " Synset('florida.n.01'),\n",
       " Synset('georgia.n.01'),\n",
       " Synset('great_lakes.n.01'),\n",
       " Synset('hawaii.n.01'),\n",
       " Synset('idaho.n.01'),\n",
       " Synset('illinois.n.01'),\n",
       " Synset('indiana.n.01'),\n",
       " Synset('iowa.n.02'),\n",
       " Synset('kansas.n.01'),\n",
       " Synset('kentucky.n.01'),\n",
       " Synset('louisiana.n.01'),\n",
       " Synset('louisiana_purchase.n.01'),\n",
       " Synset('maine.n.01'),\n",
       " Synset('maryland.n.01'),\n",
       " Synset('massachusetts.n.01'),\n",
       " Synset('michigan.n.01'),\n",
       " Synset('mid-atlantic_states.n.01'),\n",
       " Synset('midwest.n.01'),\n",
       " Synset('minnesota.n.01'),\n",
       " Synset('mississippi.n.01'),\n",
       " Synset('mississippi.n.02'),\n",
       " Synset('missouri.n.01'),\n",
       " Synset('missouri.n.02'),\n",
       " Synset('montana.n.01'),\n",
       " Synset('nebraska.n.01'),\n",
       " Synset('nevada.n.01'),\n",
       " Synset('new_england.n.01'),\n",
       " Synset('new_hampshire.n.01'),\n",
       " Synset('new_jersey.n.01'),\n",
       " Synset('new_mexico.n.01'),\n",
       " Synset('new_river.n.01'),\n",
       " Synset('new_york.n.02'),\n",
       " Synset('niagara.n.02'),\n",
       " Synset('niobrara.n.01'),\n",
       " Synset('north.n.01'),\n",
       " Synset('north_carolina.n.01'),\n",
       " Synset('north_dakota.n.01'),\n",
       " Synset('ohio.n.01'),\n",
       " Synset('ohio.n.02'),\n",
       " Synset('oklahoma.n.01'),\n",
       " Synset('oregon.n.01'),\n",
       " Synset('pacific_northwest.n.01'),\n",
       " Synset('pennsylvania.n.01'),\n",
       " Synset('rhode_island.n.01'),\n",
       " Synset('rio_grande.n.01'),\n",
       " Synset('saint_lawrence.n.02'),\n",
       " Synset('south.n.01'),\n",
       " Synset('south_carolina.n.02'),\n",
       " Synset('south_dakota.n.01'),\n",
       " Synset('sunbelt.n.01'),\n",
       " Synset('tennessee.n.01'),\n",
       " Synset('texas.n.01'),\n",
       " Synset('twin.n.03'),\n",
       " Synset('utah.n.01'),\n",
       " Synset('vermont.n.01'),\n",
       " Synset('virginia.n.01'),\n",
       " Synset('washington.n.02'),\n",
       " Synset('west.n.03'),\n",
       " Synset('west_virginia.n.01'),\n",
       " Synset('wisconsin.n.02'),\n",
       " Synset('wyoming.n.01'),\n",
       " Synset('yosemite.n.01'),\n",
       " Synset('yukon.n.01')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('united_states.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see just how intricate things can get, consider the word mint, which has several closely-related senses. We can see that mint.n.04 is part of mint.n.02 and the substance from which mint.n.05 is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'batch.n.02:', u\"(often followed by `of') a large number or amount or extent\")\n",
      "(u'mint.n.02:', u'any north temperate plant of the genus Mentha with aromatic leaves and small mauve flowers')\n",
      "(u'mint.n.03:', u'any member of the mint family of plants')\n",
      "(u'mint.n.04:', u'the leaves of a mint plant used fresh or candied')\n",
      "(u'mint.n.05:', u'a candy that is flavored with a mint oil')\n",
      "(u'mint.n.06:', u'a plant where money is coined by authority of the government')\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('mint', wn.NOUN):\n",
    "    print(synset.name() + ':', synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('mint.n.02')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('mint.n.04').part_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('mint.n.05')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('mint.n.04').substance_holonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also relationships between verbs. For example, the act of walking involves the act of stepping, so walking **entails** stepping. Some verbs have multiple entailments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('step.v.01')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('walk.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chew.v.01'), Synset('swallow.v.01')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('eat.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('arouse.v.07'), Synset('disappoint.v.01')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tease.v.03').entailments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some lexical relationships hold between lemmas, e.g., **antonymy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('demand.n.02.demand')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('supply.n.02.supply').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('linger.v.04.linger')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('rush.v.01.rush').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('inclined.a.02.inclined'), Lemma('vertical.a.01.vertical')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('horizontal.a.01.horizontal').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('legato.r.01.legato')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('staccato.r.01.staccato').antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the (numerous!) lexical relations, and the other methods defined on a synset, using dir(), for example: dir(wn.synset('harmony.n.02'))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_all_hypernyms', '_definition', '_examples', '_frame_ids', '_hypernyms', '_instance_hypernyms', '_iter_hypernym_lists', '_lemma_names', '_lemma_pointers', '_lemmas', '_lexname', '_max_depth', '_min_depth', '_name', '_needs_root', '_offset', '_pointers', '_pos', '_related', '_shortest_hypernym_paths', '_wordnet_corpus_reader', 'also_sees', 'attributes', 'causes', 'closure', 'common_hypernyms', 'definition', 'entailments', 'examples', 'frame_ids', 'hypernym_distances', 'hypernym_paths', 'hypernyms', 'hyponyms', 'instance_hypernyms', 'instance_hyponyms', 'jcn_similarity', 'lch_similarity', 'lemma_names', 'lemmas', 'lexname', 'lin_similarity', 'lowest_common_hypernyms', 'max_depth', 'member_holonyms', 'member_meronyms', 'min_depth', 'name', 'offset', 'part_holonyms', 'part_meronyms', 'path_similarity', 'pos', 'region_domains', 'res_similarity', 'root_hypernyms', 'shortest_path_distance', 'similar_tos', 'substance_holonyms', 'substance_meronyms', 'topic_domains', 'tree', 'unicode_repr', 'usage_domains', 'verb_groups', 'wup_similarity']\n"
     ]
    }
   ],
   "source": [
    "print dir(wn.synset('harmony.n.02'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic similarity\n",
    "\n",
    "We have seen that synsets are linked by a complex network of lexical relations. Given a particular synset, we can traverse the WordNet network to find synsets with related meanings. Knowing which words are semantically related is useful for indexing a collection of texts, so that a search for a general term like vehicle will match documents containing specific terms like limousine.\n",
    "\n",
    "Recall that each synset has one or more hypernym paths that link it to a root hypernym such as entity.n.01. Two synsets linked to the same root may have several hypernyms in common. If two synsets share a very specific hypernym — one that is low down in the hypernym hierarchy — they must be closely related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "right = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "novel = wn.synset('novel.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('baleen_whale.n.01')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(minke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('whale.n.02')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(orca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('vertebrate.n.01')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(tortoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we know that whale is very specific (and baleen whale even more so), while vertebrate is more general and entity is completely general. We can quantify this concept of generality by looking up the depth of each synset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwhale_minke = right.lowest_common_hypernyms(minke)\n",
    "rwhale_minke[0].min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwhale_orca = right.lowest_common_hypernyms(orca)\n",
    "rwhale_orca[0].min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwhale_vertebrate = right.lowest_common_hypernyms(tortoise)\n",
    "rwhale_vertebrate[0].min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwhale_novel = right.lowest_common_hypernyms(novel)\n",
    "rwhale_novel[0].min_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity measures have been defined over the collection of WordNet synsets which incorporate the above insight. For example, path_similarity assigns a score in the range 0–1 based on the shortest path that connects the concepts in the hypernym hierarchy (-1 is returned in those cases where a path cannot be found). Comparing a synset with itself will return 1. Consider the following similarity scores, relating right whale to minke whale, orca, tortoise, and novel. Although the numbers won't mean much, they decrease as we move away from the semantic space of sea creatures to inanimate objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right whale - Minke : 0.25\n",
      "Right whale - Orca : 0.166666666667\n",
      "Right whale - Tortoise : 0.0769230769231\n",
      "Right whale - Novel : 0.0434782608696\n"
     ]
    }
   ],
   "source": [
    "print \"Right whale - Minke :\", right.path_similarity(minke)\n",
    "print \"Right whale - Orca :\", right.path_similarity(orca)\n",
    "print \"Right whale - Tortoise :\", right.path_similarity(tortoise)\n",
    "print \"Right whale - Novel :\", right.path_similarity(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "* Find the meronyms of the human body\n",
    "* Iterate so that you can find all the meronyms of the meronyms, and so on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('homo.n.02'),\n",
       " Synset('human.a.01'),\n",
       " Synset('human.a.02'),\n",
       " Synset('human.a.03')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human = wn.synset('homo.n.02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('arm.n.01'),\n",
       " Synset('body_hair.n.01'),\n",
       " Synset('face.n.01'),\n",
       " Synset('foot.n.01'),\n",
       " Synset('hand.n.01'),\n",
       " Synset('human_body.n.01'),\n",
       " Synset('human_head.n.01'),\n",
       " Synset('loin.n.02'),\n",
       " Synset('mane.n.02')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('arm.n.01')\n",
      "Synset('arm_bone.n.01')\n",
      "Synset('biceps_brachii.n.01')\n",
      "Synset('brachial_artery.n.01')\n",
      "Synset('cephalic_vein.n.01')\n",
      "Synset('elbow.n.01')\n",
      "Synset('funny_bone.n.01')\n",
      "Synset('musculus_articularis_cubiti.n.01')\n",
      "Synset('forearm.n.01')\n",
      "Synset('accessory_cephalic_vein.n.01')\n",
      "Synset('anconeous_muscle.n.01')\n",
      "Synset('basilic_vein.n.01')\n",
      "Synset('radial_vein.n.01')\n",
      "Synset('radius.n.04')\n",
      "Synset('ulna.n.01')\n",
      "Synset('olecranon.n.01')\n",
      "Synset('ulnar_vein.n.01')\n",
      "Synset('hand.n.01')\n",
      "Synset('ball.n.10')\n",
      "Synset('digital_arteries.n.01')\n",
      "Synset('finger.n.01')\n",
      "Synset('fingernail.n.01')\n",
      "Synset('fingertip.n.01')\n",
      "Synset('knuckle.n.01')\n",
      "Synset('pad.n.07')\n",
      "Synset('intercapitular_vein.n.01')\n",
      "Synset('metacarpal_artery.n.01')\n",
      "Synset('metacarpal_vein.n.01')\n",
      "Synset('metacarpus.n.01')\n",
      "Synset('metacarpal.n.01')\n",
      "Synset('palm.n.01')\n",
      "Synset('thenar.n.01')\n",
      "Synset('humerus.n.01')\n",
      "Synset('deltoid_tuberosity.n.01')\n",
      "Synset('triceps_brachii.n.01')\n",
      "Synset('ulnar_nerve.n.01')\n",
      "Synset('wrist.n.01')\n",
      "Synset('carpal_bone.n.01')\n",
      "Synset('carpal_tunnel.n.01')\n",
      "Synset('body_hair.n.01')\n",
      "Synset('face.n.01')\n",
      "Synset('beard.n.01')\n",
      "Synset('mustache.n.01')\n",
      "Synset('brow.n.01')\n",
      "Synset('trichion.n.01')\n",
      "Synset('cheek.n.01')\n",
      "Synset('buccal_artery.n.01')\n",
      "Synset('cheek_muscle.n.01')\n",
      "Synset('chin.n.01')\n",
      "Synset('goatee.n.01')\n",
      "Synset('eye.n.01')\n",
      "Synset('aperture.n.02')\n",
      "Synset('canthus.n.01')\n",
      "Synset('central_artery_of_the_retina.n.01')\n",
      "Synset('choroid.n.01')\n",
      "Synset('ciliary_artery.n.01')\n",
      "Synset('ciliary_body.n.01')\n",
      "Synset('conjunctiva.n.01')\n",
      "Synset('cornea.n.01')\n",
      "Synset('epicanthus.n.01')\n",
      "Synset('eyeball.n.01')\n",
      "Synset('eyelid.n.01')\n",
      "Synset('conjunctiva.n.01')\n",
      "Synset('eyelash.n.01')\n",
      "Synset('iris.n.02')\n",
      "Synset('pupil.n.02')\n",
      "Synset('lacrimal_apparatus.n.01')\n",
      "Synset('lacrimal_duct.n.01')\n",
      "Synset('lacrimal_gland.n.01')\n",
      "Synset('lacrimal_sac.n.01')\n",
      "Synset('lacrimal_artery.n.01')\n",
      "Synset('lacrimal_vein.n.01')\n",
      "Synset('lens.n.04')\n",
      "Synset('lens_capsule.n.01')\n",
      "Synset('lens_cortex.n.01')\n",
      "Synset('nictitating_membrane.n.01')\n",
      "Synset('ocular_muscle.n.01')\n",
      "Synset('pupillary_sphincter.n.01')\n",
      "Synset('retina.n.01')\n",
      "Synset('blind_spot.n.02')\n",
      "Synset('cone.n.04')\n",
      "Synset('iodopsin.n.01')\n",
      "Synset('fovea.n.01')\n",
      "Synset('macula.n.02')\n",
      "Synset('neuroepithelium.n.01')\n",
      "Synset('parafovea.n.01')\n",
      "Synset('rod.n.05')\n",
      "Synset('visual_purple.n.01')\n",
      "Synset('visual_cell.n.01')\n",
      "Synset('sclera.n.01')\n",
      "Synset('uvea.n.01')\n",
      "Synset('uveoscleral_pathway.n.01')\n",
      "Synset('eyebrow.n.01')\n",
      "Synset('venae_palpebrales.n.01')\n",
      "Synset('facial.n.01')\n",
      "Synset('facial_muscle.n.01')\n",
      "Synset('facial_vein.n.01')\n",
      "Synset('feature.n.02')\n",
      "Synset('jaw.n.02')\n",
      "Synset('jowl.n.02')\n",
      "Synset('mouth.n.02')\n",
      "Synset('lingual_artery.n.01')\n",
      "Synset('lingual_vein.n.01')\n",
      "Synset('lip.n.01')\n",
      "Synset('labial_artery.n.01')\n",
      "Synset('labial_vein.n.02')\n",
      "Synset('mouth.n.01')\n",
      "Synset('buccal_cavity.n.01')\n",
      "Synset('dentition.n.02')\n",
      "Synset('gingiva.n.01')\n",
      "Synset('palate.n.01')\n",
      "Synset('hard_palate.n.01')\n",
      "Synset('soft_palate.n.01')\n",
      "Synset('uvula.n.01')\n",
      "Synset('tastebud.n.01')\n",
      "Synset('neuroepithelium.n.01')\n",
      "Synset('taste_cell.n.01')\n",
      "Synset('salivary_gland.n.01')\n",
      "Synset('saliva.n.01')\n",
      "Synset('tongue.n.01')\n",
      "Synset('tastebud.n.01')\n",
      "Synset('neuroepithelium.n.01')\n",
      "Synset('taste_cell.n.01')\n",
      "Synset('nose.n.01')\n",
      "Synset('bridge.n.04')\n",
      "Synset('nasal.n.02')\n",
      "Synset('rhinion.n.01')\n",
      "Synset('ethmoidal_artery.n.01')\n",
      "Synset('internasal_suture.n.01')\n",
      "Synset('nasal_cavity.n.01')\n",
      "Synset('neuroepithelium.n.01')\n",
      "Synset('nostril.n.01')\n",
      "Synset('turbinate_bone.n.01')\n",
      "Synset('foot.n.01')\n",
      "Synset('arcuate_artery.n.01')\n",
      "Synset('big_toe.n.01')\n",
      "Synset('digital_arteries.n.01')\n",
      "Synset('heel.n.02')\n",
      "Synset('achilles_tendon.n.01')\n",
      "Synset('heelbone.n.01')\n",
      "Synset('instep.n.01')\n",
      "Synset('intercapitular_vein.n.01')\n",
      "Synset('little_toe.n.01')\n",
      "Synset('metatarsal_artery.n.01')\n",
      "Synset('metatarsal_vein.n.01')\n",
      "Synset('sole.n.03')\n",
      "Synset('ball.n.10')\n",
      "Synset('toe.n.01')\n",
      "Synset('tiptoe.n.01')\n",
      "Synset('toenail.n.01')\n",
      "Synset('hand.n.01')\n",
      "Synset('ball.n.10')\n",
      "Synset('digital_arteries.n.01')\n",
      "Synset('finger.n.01')\n",
      "Synset('fingernail.n.01')\n",
      "Synset('fingertip.n.01')\n",
      "Synset('knuckle.n.01')\n",
      "Synset('pad.n.07')\n",
      "Synset('intercapitular_vein.n.01')\n",
      "Synset('metacarpal_artery.n.01')\n",
      "Synset('metacarpal_vein.n.01')\n",
      "Synset('metacarpus.n.01')\n",
      "Synset('metacarpal.n.01')\n",
      "Synset('palm.n.01')\n",
      "Synset('thenar.n.01')\n",
      "Synset('human_body.n.01')\n",
      "Synset('human_head.n.01')\n",
      "Synset('countenance.n.03')\n",
      "Synset('occiput.n.01')\n",
      "Synset('pate.n.02')\n",
      "Synset('scalp.n.01')\n",
      "Synset('sinciput.n.01')\n",
      "Synset('loin.n.02')\n",
      "Synset('mane.n.02')\n"
     ]
    }
   ],
   "source": [
    "def print_meronyms(synset):\n",
    "    meronyms = synset.part_meronyms()\n",
    "    if len(meronyms) == 0:\n",
    "        return\n",
    "    for part in meronyms:\n",
    "        print part\n",
    "        print_meronyms(part)\n",
    "        \n",
    "human = wn.synset('homo.n.02')\n",
    "print_meronyms(human)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some additional, semi-random examples\n",
    "\n",
    "Below are a few examples of what we can do with the NLTK toolkit, but we will discuss the details of these later in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 914 matches:\n",
      " ranging, much diffused, and common species vary most. Species of the larger ge\n",
      "used, and common species vary most. Species of the larger genera in any country\n",
      "a in any country vary more than the species of the smaller genera. Many of the \n",
      " of the smaller genera. Many of the species of the larger genera resemble varie\n",
      " same species; often severe between species of the same genus. The relation of \n",
      "condary sexual characters variable. Species of the same genus vary in an analog\n",
      "rsified habits in the same species. Species with habits widely different from t\n",
      "On their different rates of change. Species once lost do not reappear. Groups o\n",
      "nce lost do not reappear. Groups of species follow the same general rules in th\n",
      "world. On the affinities of extinct species to each other and to living species\n",
      "ht come to the conclusion that each species had not been independently created,\n",
      " could be shown how the innumerable species inhabiting this world have been mod\n",
      " then pass on to the variability of species in a state of nature; but I shall, \n",
      "s. As many more individuals of each species are born than can possibly survive;\n",
      "y, Hybridism, or the infertility of species and the fertility of varieties when\n",
      "xplained in regard to the origin of species and varieties, if he makes due allo\n",
      " around us. Who can explain why one species ranges widely and is very numerous,\n",
      "ry numerous, and why another allied species has a narrow range and is rare? Yet\n",
      "erly entertained--namely, that each species has been independently created--is \n",
      "rroneous. I am fully convinced that species are not immutable; but that those b\n",
      "e acknowledged varieties of any one species are the descendants of that species\n",
      " than do the individuals of any one species or variety in a state of nature. Wh\n",
      "can be drawn from domestic races to species in a state of nature. I have in vai\n",
      "s and plants, and compare them with species closely allied together, we general\n",
      "from each other, and from the other species of the same genus, in several trifl\n"
     ]
    }
   ],
   "source": [
    "# Examples of the appearance of the word \"selection\"\n",
    "text.concordance(\"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural selection; organic beings; natural selection,; closely allied;\n",
      "natural selection.; South America,; New Zealand,; secondary sexual;\n",
      "organic beings,; physical conditions; United States,; widely\n",
      "different; North America; closely related; one species; Dr. Hooker;\n",
      "United States; Tierra del; non facit; Glacial period,\n"
     ]
    }
   ],
   "source": [
    "# Frequent collocations in the text (usually meaningful phrases)\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dispersion plot\n",
    "text.dispersion_plot([\"selection\", \"species\", \"organic\", \"allied\", \"parent\", \"sexual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* A frequency distribution is a collection of items along with their frequency counts (e.g., the words of a text and their frequency of appearance).\n",
    "* Tokenization is the segmentation of a text into basic units — or tokens — such as words and punctuation. Tokenization based on whitespace is inadequate for many applications because it bundles punctuation together with words. NLTK provides an off-the-shelf tokenizer nltk.word_tokenize().\n",
    "* Stemming is the process of removing the affix of a word, to create a \"normalized\" representation of the token\n",
    "* Lemmatization is a process that maps the various forms of a word (such as appeared, appears) to the canonical or citation form of the word, also known as the lexeme or lemma (e.g. appear). \n",
    "* WordNet is a semantically-oriented dictionary of English, consisting of synonym sets — or synsets — and organized into a network.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
